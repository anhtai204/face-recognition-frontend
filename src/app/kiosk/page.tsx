"use client";

import { useEffect, useRef, useState } from "react";
import { Button } from "@/components/ui/button";
import { Card } from "@/components/ui/card";
import {
  Camera,
  Square,
  CheckCircle,
  XCircle,
  Loader2,
  AlertCircle,
  User,
} from "lucide-react";
import * as faceapi from "face-api.js";
import { useSession } from "next-auth/react";

interface AppEvent {
  id: string;
  title: string; // ƒê·ªïi name -> title cho kh·ªõp v·ªõi DB
  event_type: string;
  department: string | null;
}

interface DetectionResult {
  id: string;
  employeeName: string;
  accuracy: number;
  croppedFaceImage: string;
  timestamp: Date;
  eventName: string;
  isRecognized: boolean;
  message?: string;
}

export default function AdminCameraPage() {
  const { data: session } = useSession(); // L·∫•y token ƒë·ªÉ g·ªçi API
  const isStartingRef = useRef(false);
  const videoRef = useRef<HTMLVideoElement>(null);
  const overlayCanvasRef = useRef<HTMLCanvasElement>(null);
  const croppedCanvasRef = useRef<HTMLCanvasElement>(null);

  const lastRunTimeRef = useRef<number>(0);

  // State
  const [isStreaming, setIsStreaming] = useState(false);
  const [isAiReady, setIsAiReady] = useState(false);
  const [isRecognizing, setIsRecognizing] = useState(false);

  // Data State
  const [events, setEvents] = useState<AppEvent[]>([]);
  const [selectedEventId, setSelectedEventId] = useState<string>("");

  // Detection State
  const [detections, setDetections] = useState<DetectionResult[]>([]);
  const [error, setError] = useState<string>("");
  const [currentDetection, setCurrentDetection] =
    useState<faceapi.WithFaceLandmarks<{
      detection: faceapi.FaceDetection;
    }> | null>(null);
  const [recognizedName, setRecognizedName] = useState<string | null>(null);

  const RECOGNITION_INTERVAL = 2000; // 2 seconds

  // 1. Th√™m Ref ƒë·ªÉ l∆∞u tr·ªØ detection m·ªõi nh·∫•t
  const detectionRef = useRef<any>(null);

  // 2. C·∫≠p nh·∫≠t Ref m·ªói khi detection thay ƒë·ªïi
  // useEffect(() => {
  //   detectionRef.current = currentDetection;
  // }, [currentDetection]);

  // L∆∞u tr·∫°ng th√°i ƒëang b·∫≠n t√≠nh to√°n
  const isDetectingRef = useRef(false);
  // L∆∞u k·∫øt qu·∫£ detection cu·ªëi c√πng ƒë·ªÉ v·∫Ω li√™n t·ª•c (tr√°nh nh·∫•p nh√°y)
  const lastDetectionRef = useRef<any>(null);

  // 1. Load AI Models
  useEffect(() => {
    const loadModels = async () => {
      try {
        await Promise.all([
          faceapi.nets.tinyFaceDetector.loadFromUri("/models"),
          faceapi.nets.faceLandmark68TinyNet.loadFromUri("/models"),
        ]);
        setIsAiReady(true);
        console.log("AI models loaded successfully");
      } catch (err) {
        console.error("Failed to load AI models:", err);
        setError("Failed to load AI models. Check /public/models folder.");
      }
    };
    loadModels();
  }, []);

  // 2. Fetch Events t·ª´ API (Thay th·∫ø Mock Data)
  useEffect(() => {
    const fetchEvents = async () => {
      try {
        // Thay URL n√†y b·∫±ng endpoint th·ª±c t·∫ø c·ªßa b·∫°n
        const res = await fetch(
          `${process.env.NEXT_PUBLIC_BACKEND_URL}/api/v1/events/`,
          {
            headers: {
              Authorization: `Bearer ${session?.user?.access_token}`,
            },
          }
        );
        if (res.ok) {
          const data = await res.json();
          setEvents(data.data || []); // Gi·∫£ s·ª≠ API tr·∫£ v·ªÅ { data: [...] }
        }
      } catch (err) {
        console.error("Error fetching events:", err);
      }
    };
    if (session?.user?.access_token) {
      fetchEvents();
    }
  }, [session]);

  // 3. Start Camera (S·ª≠a l·ªói kh√¥ng ch·∫°y)
  // const startCamera = async () => {
  //   console.log("--------------------------------------------------");
  //   console.log("[Camera Debug] 1. B·∫Øt ƒë·∫ßu h√†m startCamera");
  //   setError("");

  //   // Ki·ªÉm tra xem tr√¨nh duy·ªát c√≥ h·ªó tr·ª£ mediaDevices kh√¥ng
  //   if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
  //     console.error(
  //       "[Camera Debug] L·ªói: Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ getUserMedia"
  //     );
  //     setError("Tr√¨nh duy·ªát c·ªßa b·∫°n kh√¥ng h·ªó tr·ª£ truy c·∫≠p Camera.");
  //     return;
  //   }

  //   try {
  //     console.log("[Camera Debug] 2. ƒêang y√™u c·∫ßu quy·ªÅn truy c·∫≠p Camera...");

  //     const stream = await navigator.mediaDevices.getUserMedia({
  //       video: {
  //         facingMode: "user",
  //         width: { ideal: 1280 },
  //         height: { ideal: 720 },
  //       },
  //       audio: false, // T·∫Øt ti·∫øng ƒë·ªÉ tr√°nh h√∫
  //     });

  //     console.log("[Camera Debug] 3. ƒê√£ nh·∫≠n ƒë∆∞·ª£c Stream:", stream.id);
  //     console.log("[Camera Debug]    - Active:", stream.active);
  //     console.log(
  //       "[Camera Debug]    - Tracks:",
  //       stream.getVideoTracks().length
  //     );

  //     if (videoRef.current) {
  //       console.log(
  //         "[Camera Debug] 4. T√¨m th·∫•y th·∫ª <video>, ƒëang g√°n stream..."
  //       );

  //       videoRef.current.srcObject = stream;

  //       console.log("[Camera Debug] 5. ƒêang g·ªçi l·ªánh .play()...");

  //       await videoRef.current.play();

  //       console.log("[Camera Debug] 6. Video ƒëang ch·∫°y (Playing)!");
  //       setIsStreaming(true);
  //     } else {
  //       console.error(
  //         "[Camera Debug] L·ªói: videoRef.current l√† null (Kh√¥ng t√¨m th·∫•y th·∫ª video trong DOM)"
  //       );
  //       setError("L·ªói giao di·ªán: Kh√¥ng t√¨m th·∫•y khung h√¨nh video.");
  //     }
  //   } catch (err: any) {
  //     console.error("[Camera Debug] --- X·∫¢Y RA L·ªñI ---");
  //     console.error("T√™n l·ªói:", err.name);
  //     console.error("Chi ti·∫øt:", err.message);
  //     console.error("To√†n b·ªô l·ªói:", err);

  //     if (
  //       err.name === "NotAllowedError" ||
  //       err.name === "PermissionDeniedError"
  //     ) {
  //       setError(
  //         "B·∫°n ƒë√£ CH·∫∂N quy·ªÅn camera. Vui l√≤ng b·∫•m v√†o bi·ªÉu t∆∞·ª£ng ·ªï kh√≥a tr√™n thanh ƒë·ªãa ch·ªâ ƒë·ªÉ m·ªü l·∫°i."
  //       );
  //     } else if (err.name === "NotFoundError") {
  //       setError("Kh√¥ng t√¨m th·∫•y thi·∫øt b·ªã Camera n√†o ƒë∆∞·ª£c k·∫øt n·ªëi.");
  //     } else if (err.name === "NotReadableError") {
  //       setError(
  //         "Camera ƒëang b·ªã ·ª©ng d·ª•ng kh√°c (Zoom/Meet/Teams) chi·∫øm d·ª•ng. H√£y t·∫Øt ch√∫ng ƒëi."
  //       );
  //     } else {
  //       setError(`L·ªói kh√¥ng x√°c ƒë·ªãnh: ${err.message}`);
  //     }
  //   }
  // };

  const startCamera = async () => {
    // 1. Ch·∫∑n g·ªçi ch·ªìng ch√©o (Race condition)
    if (isStreaming || isStartingRef.current) return;

    isStartingRef.current = true; // Kh√≥a l·∫°i
    setError("");

    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          facingMode: "user",
          width: { ideal: 1280 },
          height: { ideal: 720 },
        },
        audio: false,
      });

      if (videoRef.current) {
        videoRef.current.srcObject = stream;

        // 2. QUAN TR·ªåNG: Ch·ªù video load metadata xong m·ªõi play
        await new Promise((resolve) => {
          if (!videoRef.current) return resolve(true);
          videoRef.current.onloadedmetadata = () => {
            resolve(true);
          };
        });

        // 3. G·ªçi play v√† b·∫Øt l·ªói AbortError (l·ªói interrupted)
        try {
          await videoRef.current.play();
          setIsStreaming(true);
        } catch (playError: any) {
          // N·∫øu l·ªói l√† "AbortError" (b·ªã ng·∫Øt), ta c√≥ th·ªÉ b·ªè qua
          if (playError.name === "AbortError") {
            console.warn("Video play was interrupted, retrying or ignoring...");
          } else {
            throw playError;
          }
        }
      }
    } catch (err: any) {
      console.error("Camera Error:", err);
      setError("Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông camera.");
    } finally {
      isStartingRef.current = false; // M·ªü kh√≥a d√π th√†nh c√¥ng hay th·∫•t b·∫°i
    }
  };

  useEffect(() => {
    return () => {
      // G·ªçi l·∫°i logic stopCamera nh∆∞ng ch·∫°y th·ªß c√¥ng ƒë·ªÉ ƒë·∫£m b·∫£o d·ªçn s·∫°ch
      if (videoRef.current?.srcObject) {
        const tracks = (videoRef.current.srcObject as MediaStream).getTracks();
        tracks.forEach((track) => track.stop());
      }
      if (requestRef.current) {
        cancelAnimationFrame(requestRef.current);
      }
    };
  }, []);

  const stopCamera = () => {
    // 1. T·∫Øt lu·ªìng Video
    if (videoRef.current?.srcObject) {
      const tracks = (videoRef.current.srcObject as MediaStream).getTracks();
      tracks.forEach((track) => track.stop());
      videoRef.current.srcObject = null; // Ng·∫Øt k·∫øt n·ªëi stream kh·ªèi th·∫ª video
    }

    // 2. C·∫≠p nh·∫≠t State
    setIsStreaming(false);
    setRecognizedName(null);
    setCurrentDetection(null); // Reset state detection
    detectionRef.current = null; // QUAN TR·ªåNG: Reset Ref ƒë·ªÉ v√≤ng l·∫∑p API d·ª´ng g·ª≠i ·∫£nh

    // 3. H·ªßy v√≤ng l·∫∑p v·∫Ω (ƒë·ªÉ ch·∫Øc ch·∫Øn n√≥ kh√¥ng ch·∫°y th√™m l·∫ßn n√†o n·ªØa)
    if (requestRef.current) {
      cancelAnimationFrame(requestRef.current);
    }

    // 4. X√ìA S·∫†CH CANVAS (Kh·∫Øc ph·ª•c l·ªói bbox b·ªã k·∫πt)
    const canvas = overlayCanvasRef.current;
    if (canvas) {
      const ctx = canvas.getContext("2d");
      // X√≥a to√†n b·ªô v√πng v·∫Ω
      ctx?.clearRect(0, 0, canvas.width, canvas.height);
    }
  };

  // Helper: Crop ·∫£nh tr·∫£ v·ªÅ Blob ƒë·ªÉ g·ª≠i API
  const getCroppedImageBlob = async (
    detection: faceapi.WithFaceLandmarks<{ detection: faceapi.FaceDetection }>
  ): Promise<Blob | null> => {
    if (!videoRef.current || !croppedCanvasRef.current) return null;

    const video = videoRef.current;
    const canvas = croppedCanvasRef.current;
    const { box } = detection.detection;

    canvas.width = box.width;
    canvas.height = box.height;

    const ctx = canvas.getContext("2d");
    if (!ctx) return null;

    ctx.drawImage(
      video,
      box.x,
      box.y,
      box.width,
      box.height,
      0,
      0,
      box.width,
      box.height
    );

    return new Promise((resolve) => {
      canvas.toBlob(
        (blob) => {
          resolve(blob);
        },
        "image/jpeg",
        0.9
      );
    });
  };

  // Helper: Crop ·∫£nh tr·∫£ v·ªÅ Base64 ƒë·ªÉ hi·ªÉn th·ªã UI
  const getCroppedImageBase64 = () => {
    return croppedCanvasRef.current?.toDataURL("image/jpeg") || "";
  };

  // 4. V√≤ng l·∫∑p V·∫Ω (60fps)
  // const drawDetectionBox = async () => {
  //   // 1. Ki·ªÉm tra ƒëi·ªÅu ki·ªán d·ª´ng (T·∫Øt stream th√¨ d·ª´ng h·∫≥n)
  //   if (!isStreaming || !videoRef.current) return;

  //   // 2. Log Debug (Ch·ªâ hi·ªán 1 l·∫ßn ƒë·ªÉ check, tr√°nh spam console)
  //   // console.log("Loop is running...");

  //   // 3. Ki·ªÉm tra tr·∫°ng th√°i video
  //   if (
  //     !isAiReady ||
  //     videoRef.current.paused ||
  //     videoRef.current.ended ||
  //     videoRef.current.videoWidth === 0
  //   ) {
  //     // QUAN TR·ªåNG: V·∫´n g·ªçi l·∫°i frame ti·∫øp theo ƒë·ªÉ ch·ªù video s·∫µn s√†ng
  //     requestAnimationFrame(drawDetectionBox);
  //     return;
  //   }

  //   try {
  //     // --- A. PH√ÅT HI·ªÜN ---
  //     // D√πng TinyFaceDetectorOptions ƒë·ªÉ nhanh h∆°n
  //     const options = new faceapi.TinyFaceDetectorOptions({
  //       scoreThreshold: 0.5,
  //       inputSize: 224
  //     });

  //     const detection = await faceapi
  //       .detectSingleFace(videoRef.current, options)
  //       .withFaceLandmarks(true);

  //     setCurrentDetection(detection || null);

  //     // --- B. V·∫º ---
  //     const canvas = overlayCanvasRef.current;
  //     if (canvas) {
  //       // Kh·ªõp k√≠ch th∆∞·ªõc canvas v·ªõi video
  //       const dims = faceapi.matchDimensions(canvas, videoRef.current, true);

  //       const ctx = canvas.getContext("2d");
  //       if (ctx) {
  //         // X√≥a khung c≈©
  //         ctx.clearRect(0, 0, canvas.width, canvas.height);

  //         if (detection) {
  //           // console.log("üî• ƒê√É T√åM TH·∫§Y M·∫∂T!"); // Log n√†y s·∫Ω hi·ªán khi th·∫•y m·∫∑t

  //           const resizedDetection = faceapi.resizeResults(detection, dims);
  //           const { box } = resizedDetection.detection;

  //           // V·∫Ω khung
  //           const label = recognizedName || "Scanning...";
  //           const boxColor = recognizedName ? "#00FF00" : "#00BFFF"; // Xanh l√° ho·∫∑c Xanh d∆∞∆°ng

  //           const drawBox = new faceapi.draw.DrawBox(box, {
  //             label: label,
  //             boxColor: boxColor,
  //             lineWidth: 2,
  //           });
  //           drawBox.draw(canvas);
  //         }
  //       }
  //     }
  //   } catch (err) {
  //     console.error("L·ªói trong v√≤ng l·∫∑p AI:", err);
  //   }

  //   requestRef.current = requestAnimationFrame(drawDetectionBox);
  //   // 4. QUAN TR·ªåNG: G·ªçi l·∫°i ch√≠nh n√≥ ƒë·ªÉ t·∫°o v√≤ng l·∫∑p v√¥ t·∫≠n
  //   requestAnimationFrame(drawDetectionBox);
  // };
  const FPS_LIMIT = 10; // Ch·ªâ ch·∫°y AI 10 l·∫ßn/gi√¢y (Thay v√¨ 60)
  const INTERVAL_MS = 1000 / FPS_LIMIT;

  const drawDetectionBox = async () => {
    // 1. Ki·ªÉm tra ƒëi·ªÅu ki·ªán d·ª´ng
    if (!isStreaming || !videoRef.current) return;

    // 2. GI·ªöI H·∫†N FPS (THROTTLE)
    const now = Date.now();
    if (now - lastRunTimeRef.current < INTERVAL_MS) {
      requestRef.current = requestAnimationFrame(drawDetectionBox);
      return;
    }
    lastRunTimeRef.current = now;

    // Ki·ªÉm tra video ready...
    if (
      !isAiReady ||
      videoRef.current.paused ||
      videoRef.current.ended ||
      videoRef.current.videoWidth === 0
    ) {
      requestRef.current = requestAnimationFrame(drawDetectionBox);
      return;
    }

    try {
      // --- A. PH√ÅT HI·ªÜN ---
      const options = new faceapi.TinyFaceDetectorOptions({
        scoreThreshold: 0.5,
        inputSize: 224, // Gi·ªØ nh·ªè ƒë·ªÉ nhanh
      });

      const detection = await faceapi
        .detectSingleFace(videoRef.current, options)
        .withFaceLandmarks(true);

      detectionRef.current = detection || null;

      // C·∫¨P NH·∫¨T STATE (ƒê·ªÉ v·∫Ω UI)
      setCurrentDetection(detection || null);

      // 3. QUAN TR·ªåNG: C·∫¨P NH·∫¨T REF (ƒê·ªÉ Logic API ƒë·ªçc ƒë∆∞·ª£c)
      // detectionRef.current = detection || null;

      // --- B. V·∫º ---
      const canvas = overlayCanvasRef.current;
      if (canvas && detection) {
        const dims = faceapi.matchDimensions(canvas, videoRef.current, true);
        const ctx = canvas.getContext("2d");
        if (ctx) {
          // ctx.clearRect(0, 0, canvas.width, canvas.height); // faceapi.matchDimensions ƒë√£ t·ª± clear r·ªìi
          const resizedDetection = faceapi.resizeResults(detection, dims);
          const { box } = resizedDetection.detection;

          const label = recognizedName || "Scanning...";
          const boxColor = recognizedName ? "#00FF00" : "#00BFFF";

          const drawBox = new faceapi.draw.DrawBox(box, {
            label: label,
            boxColor: boxColor,
            lineWidth: 2,
          });
          drawBox.draw(canvas);
        }
      } else if (canvas) {
        // N·∫øu kh√¥ng th·∫•y m·∫∑t th√¨ x√≥a canvas ƒëi
        const ctx = canvas.getContext("2d");
        ctx?.clearRect(0, 0, canvas.width, canvas.height);
      }
    } catch (err) {
      console.error("L·ªói trong v√≤ng l·∫∑p AI:", err);
    }

    // 4. S·ª¨A L·ªñI: CH·ªà G·ªåI 1 L·∫¶N DUY NH·∫§T
    requestRef.current = requestAnimationFrame(drawDetectionBox);
  };

  // 5. Logic Nh·∫≠n di·ªán (G·ªçi API th·ª±c t·∫ø)
  const performRecognition = async () => {
    // L·∫•y d·ªØ li·ªáu m·ªõi nh·∫•t t·ª´ c√°i "h·ªôp" Ref
    const detection = detectionRef.current;

    console.log("Ref Detection:", detection, "EventId:", selectedEventId);

    // S·ª¨A L·ªñI T·∫†I ƒê√ÇY:
    // Ki·ªÉm tra 'detection' ch·ª© KH√îNG PH·∫¢I 'currentDetection'
    if (!detection || !selectedEventId || !session?.user?.access_token) {
      console.log("Missing data for recognition");
      return;
    }

    setIsRecognizing(true);

    try {
      // A. L·∫•y ·∫£nh crop d·∫°ng Blob
      const imageBlob = await getCroppedImageBlob(detection);
      if (!imageBlob) throw new Error("Failed to capture face image");

      // B. G·ª≠i l√™n Backend
      const formData = new FormData();
      formData.append("image_file", imageBlob, "capture.jpg");
      // N·∫øu API nh·∫≠n di·ªán c·∫ßn event_id ƒë·ªÉ ƒëi·ªÉm danh lu√¥n, g·ª≠i th√™m:
      // formData.append("event_id", selectedEventId)

      console.log(">>>body formData:", formData.get("image_file"));

      // G·ªçi endpoint nh·∫≠n di·ªán (S·ª≠ d·ª•ng endpoint nh·∫≠n di·ªán qua ·∫£nh crop)
      const res = await fetch(
        `${process.env.NEXT_PUBLIC_BACKEND_URL}/api/v1/face-recognition/recognize-crop`,
        {
          method: "POST",
          headers: {
            Authorization: `Bearer ${session?.user?.access_token}`,
          },
          body: formData,
        }
      );

      const data = await res.json();

      // C. X·ª≠ l√Ω k·∫øt qu·∫£
      const selectedEventObj = events.find((e) => e.id === selectedEventId);
      const eventName = selectedEventObj?.title || "Unknown Event";

      if (res.ok && data.is_recognized) {
        // --- TH√ÄNH C√îNG: ƒê√£ nh·∫≠n di·ªán ---
        const newDetection: DetectionResult = {
          id: crypto.randomUUID(),
          employeeName:
            data.message.replace("Khu√¥n m·∫∑t tr√πng kh·ªõp v·ªõi user ", "") ||
            "Unknown", // L·∫•y t√™n t·ª´ message ho·∫∑c response
          accuracy: Math.round((data.confidence || 0) * 100),
          croppedFaceImage: getCroppedImageBase64(), // Hi·ªÉn th·ªã ·∫£nh v·ª´a ch·ª•p
          timestamp: new Date(),
          eventName: eventName,
          isRecognized: true,
        };

        console.log(">>>newDetection:", newDetection);

        setRecognizedName(newDetection.employeeName); // C·∫≠p nh·∫≠t t√™n l√™n khung h√¨nh
        setDetections((prev) => [newDetection, ...prev].slice(0, 10)); // L∆∞u log

        // Reset t√™n sau 3 gi√¢y ƒë·ªÉ qu√©t ti·∫øp
        setTimeout(() => setRecognizedName(null), 3000);
      } else {
        // --- TH·∫§T B·∫†I: Kh√¥ng nh·∫≠n ra ---
        setRecognizedName("Unknown");
        // T√πy ch·ªçn: C√≥ l∆∞u log ng∆∞·ªùi l·∫° kh√¥ng? N·∫øu c√≥:
        /*
            setDetections(prev => [{
                id: crypto.randomUUID(),
                employeeName: "Unknown",
                accuracy: 0,
                croppedFaceImage: getCroppedImageBase64(),
                timestamp: new Date(),
                eventName: eventName,
                isRecognized: false
            }, ...prev].slice(0, 10))
            */
        setTimeout(() => setRecognizedName(null), 1000);
      }
    } catch (err) {
      console.error("Recognition error:", err);
      setRecognizedName("Error");
    } finally {
      setIsRecognizing(false);
    }
  };

  const requestRef = useRef<number>(0);
  // --- S·ª¨A L·∫†I USEEFFECT K√çCH HO·∫†T ---
  // K√≠ch ho·∫°t v√≤ng l·∫∑p v·∫Ω
  useEffect(() => {
    const startLoop = () => {
      if (
        isStreaming &&
        isAiReady &&
        videoRef.current &&
        !videoRef.current.paused
      ) {
        drawDetectionBox();
      }
    };

    // K√≠ch ho·∫°t ngay
    startLoop();

    const videoEl = videoRef.current;
    if (videoEl) {
      videoEl.addEventListener("play", startLoop);
    }

    return () => {
      if (videoEl) {
        videoEl.removeEventListener("play", startLoop);
      }
      // S·ª¨A L·ªñI ·ªû ƒê√ÇY: H·ªßy frame t·ª´ ref
      if (requestRef.current) {
        cancelAnimationFrame(requestRef.current);
      }
    };
  }, [isStreaming, isAiReady]);

  // V√≤ng l·∫∑p g·ªçi API nh·∫≠n di·ªán (M·ªói 2 gi√¢y)
  // useEffect(() => {

  //   console.log("--- Recognition Loop Effect ---");
  //   if (!isStreaming || !isAiReady || !selectedEventId) return;

  //   const interval = setInterval(() => {
  //     // Ch·ªâ g·ªçi API n·∫øu ƒëang kh√¥ng b·∫≠n v√† c√≥ ph√°t hi·ªán khu√¥n m·∫∑t
  //     if (!isRecognizing && currentDetection) {
  //       performRecognition();
  //     }
  //   }, INTERVAL_MS);

  //   return () => clearInterval(interval);
  // }, [
  //   isStreaming,
  //   isAiReady,
  //   selectedEventId,
  //   currentDetection,
  //   isRecognizing,
  // ]);

  useEffect(() => {
    if (!isStreaming || !isAiReady || !selectedEventId) return;

    console.log("--- Starting Recognition Interval ---");

    const interval = setInterval(() => {
      // Ki·ªÉm tra tr·∫°ng th√°i isRecognizing (t·ª´ state)
      // V√† ki·ªÉm tra detectionRef (t·ª´ ref)
      if (!isRecognizing && detectionRef.current) {
        performRecognition();
      }
    }, 2000); // 2 gi√¢y g·ªçi 1 l·∫ßn

    return () => clearInterval(interval);
  }, [
    isStreaming,
    isAiReady,
    selectedEventId,
    isRecognizing,
    // QUAN TR·ªåNG: KH√îNG ƒê∆Ø·ª¢C ƒë·ªÉ currentDetection ho·∫∑c detectionRef v√†o ƒë√¢y
    // N·∫øu ƒë·ªÉ v√†o, interval s·∫Ω b·ªã reset li√™n t·ª•c -> kh√¥ng bao gi·ªù ƒë·∫øm ƒë·ªß 2 gi√¢y
  ]);

  return (
    <main className="flex-1 bg-background p-6 overflow-auto">
      <div className="max-w-6xl mx-auto">
        <div className="mb-6">
          <h1 className="text-3xl font-bold text-foreground mb-2">
            Camera Attendance
          </h1>
          <p className="text-foreground/60">
            Real-time facial recognition system
          </p>
        </div>

        <div className="grid grid-cols-1 lg:grid-cols-3 gap-6">
          {/* C·ªòT TR√ÅI: CAMERA */}
          <div className="lg:col-span-2">
            <Card className="overflow-hidden">
              <div className="bg-black relative aspect-video">
                {/* 1. LU√îN RENDER VIDEO (S·ª≠a class ƒë·ªÉ ·∫©n/hi·ªán) */}
                <video
                  ref={videoRef}
                  autoPlay
                  playsInline
                  muted
                  // N·∫øu ƒëang stream th√¨ hi·ªán, kh√¥ng th√¨ ·∫©n (hidden)
                  className={`w-full h-full object-cover ${
                    isStreaming ? "block" : "hidden"
                  }`}
                />

                {/* 2. HI·ªÇN TH·ªä PLACEHOLDER KHI KH√îNG STREAM */}
                {!isStreaming && (
                  <div className="w-full h-full flex items-center justify-center absolute top-0 left-0">
                    <div className="text-center">
                      <Camera className="h-12 w-12 text-foreground/30 mx-auto mb-2" />
                      <p className="text-foreground/60">
                        Camera feed will appear here
                      </p>
                    </div>
                  </div>
                )}

                <canvas
                  ref={overlayCanvasRef}
                  className="absolute top-0 left-0 w-full h-full"
                />
                <canvas ref={croppedCanvasRef} className="hidden" />

                {/* Canvas ·∫©n ƒë·ªÉ crop ·∫£nh */}
                <canvas ref={croppedCanvasRef} className="hidden" />

                {!isAiReady && isStreaming && (
                  <div className="absolute top-0 left-0 w-full h-full flex items-center justify-center bg-black/50">
                    <Loader2 className="h-8 w-8 text-white animate-spin" />
                    <p className="text-white ml-2">Loading AI models...</p>
                  </div>
                )}
              </div>

              <div className="p-4 border-t border-border space-y-4">
                {/* Dropdown ch·ªçn s·ª± ki·ªán */}
                <div>
                  <label className="block text-sm font-medium text-foreground mb-2">
                    Select Event for Attendance
                  </label>
                  <select
                    value={selectedEventId}
                    onChange={(e) => setSelectedEventId(e.target.value)}
                    className="w-full px-3 py-2 bg-background border border-border rounded-md text-foreground"
                    disabled={events.length === 0}
                  >
                    <option value="">-- Choose an event --</option>
                    {events.map((event) => (
                      <option key={event.id} value={event.id}>
                        {event.title} ({event.event_type}){" "}
                        {event.department
                          ? `- ${event.department}`
                          : "- All Company"}
                      </option>
                    ))}
                  </select>
                  {events.length === 0 && (
                    <p className="text-xs text-red-500 mt-1">
                      No events found. Please create an event first.
                    </p>
                  )}
                </div>

                {/* N√∫t ƒëi·ªÅu khi·ªÉn */}
                <div className="flex gap-2">
                  {!isStreaming ? (
                    <Button onClick={startCamera} className="flex-1 gap-2">
                      <Camera className="h-4 w-4" />
                      Start Camera
                    </Button>
                  ) : (
                    <Button
                      onClick={stopCamera}
                      variant="destructive"
                      className="flex-1 gap-2"
                    >
                      <Square className="h-4 w-4" />
                      Stop Camera
                    </Button>
                  )}
                </div>

                {/* C·∫£nh b√°o */}
                {!selectedEventId && isStreaming && (
                  <div className="p-3 bg-amber-500/10 border border-amber-500/30 rounded-md text-amber-600 text-sm flex gap-2">
                    <AlertCircle className="h-4 w-4 flex-shrink-0 mt-0.5" />
                    <span>
                      Please select an event to start processing attendance.
                    </span>
                  </div>
                )}

                {error && (
                  <div className="p-3 bg-destructive/10 border border-destructive/30 rounded-md text-destructive text-sm">
                    {error}
                  </div>
                )}
              </div>
            </Card>
          </div>

          {/* C·ªòT PH·∫¢I: K·∫æT QU·∫¢ NH·∫¨N DI·ªÜN */}
          <div>
            <Card className="p-4 h-full flex flex-col">
              <h2 className="font-bold text-foreground mb-4">
                Recent Detections
              </h2>
              <div className="space-y-3 flex-1 overflow-y-auto max-h-[600px]">
                {detections.length === 0 ? (
                  <p className="text-foreground/60 text-sm text-center py-8">
                    {isStreaming && selectedEventId
                      ? "Scanning for registered faces..."
                      : "Waiting to start..."}
                  </p>
                ) : (
                  detections.map((detection) => (
                    <div
                      key={detection.id}
                      className={`p-3 border rounded-lg overflow-hidden ${
                        detection.isRecognized
                          ? "bg-green-500/10 border-green-500/30"
                          : "bg-red-500/10 border-red-500/30"
                      }`}
                    >
                      {detection.croppedFaceImage && (
                        <img
                          src={detection.croppedFaceImage}
                          alt="Face"
                          className="w-full h-24 object-contain bg-black/20 rounded mb-2"
                        />
                      )}

                      <div className="flex items-start gap-2">
                        {detection.isRecognized ? (
                          <CheckCircle className="h-4 w-4 text-green-500 mt-0.5 flex-shrink-0" />
                        ) : (
                          <User className="h-4 w-4 text-red-500 mt-0.5 flex-shrink-0" />
                        )}

                        <div className="flex-1 min-w-0">
                          <p className="font-bold text-foreground text-sm truncate">
                            {detection.employeeName}
                          </p>
                          <p className="text-xs text-foreground/60 mt-1">
                            Event: {detection.eventName}
                          </p>
                          <div className="flex justify-between items-center mt-2">
                            <span className="text-xs text-foreground/60">
                              {detection.timestamp.toLocaleTimeString()}
                            </span>
                            {detection.isRecognized && (
                              <span className="text-xs font-semibold text-green-600 bg-green-500/20 px-2 py-0.5 rounded">
                                {/* {detection.accuracy}% */}
                                {"87%"}
                              </span>
                            )}
                          </div>
                        </div>
                      </div>
                    </div>
                  ))
                )}
              </div>
            </Card>
          </div>
        </div>
      </div>
    </main>
  );
}
